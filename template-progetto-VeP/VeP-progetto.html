<!DOCTYPE html>
<!-- VeP http://web.unibas.it/bloisi/corsi/visione-e-percezione.html -->
<html lang="en"><!--<![endif]--><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>3D pose estimation</title>
	<!-- Meta -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Descrizione del progetto">
    <meta name="author" content="Domenico Bloisi adapted a 3rd Wave Media template">    
    <link rel="shortcut icon" href="http://web.unibas.it/bloisi/tutorial/favicon.ico">  
    <link href="VeP-progetto_files/css.txt" rel="stylesheet" type="text/css">
    <link href="VeP-progetto_files/css1.txt" rel="stylesheet" type="text/css"> 
    <!-- Global CSS -->
    <link rel="stylesheet" href="VeP-progetto_files/bootstrap.css">   
    <!-- Plugins CSS -->
    <link rel="stylesheet" href="VeP-progetto_files/font-awesome.css">
        
    <!-- Theme CSS -->  
    <link id="theme-style" rel="stylesheet" href="VeP-progetto_files/styles.css">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->
    
</head> 

<body>
    <!-- ******HEADER****** --> 
    <header class="header">
        <div class="container" >                       
            <img class="profile-image img-responsive pull-left" src="image/franka.jpeg" alt="franka" hegiht="200" width="200">
            <div class="profile-content pull-left">
                <h1 class="name" align = "center">3D pose estimation</h1>
                <h2 class="desc" align = "center">Stima della posizione con MediaPipe <br> e simulazione del grasping con Franka Emika Panda</h2>
            </div>	

            <div class="profile-content pull-right">
				<img class="profile-image img-responsive pull-left"
				    src="http://web.unibas.it/bloisi/assets/images/logo.png"
					alt="unibas logo"
					height=97 width=312/>
				
				<p>&nbsp;</p>
				<h3 class="desc">
				<a href="http://web.unibas.it/bloisi/corsi/visione-e-percezione.html"
				target="_blank">
				Corso di Visione e Percezione</a>
				</h3>
			</div>
			
        </div><!--//container-->
    </header><!--//header-->
    
    <div class="container sections-wrapper">
        <div class="row">
            <div class="primary col-md-8 col-sm-12 col-xs-12">
			
			    <section class="about section">
                    <div class="section-inner">
                        <h2 class="heading">Problema</h2>
                        <div class="content">
                            <p align = "justify">Rilevamento di oggetti in immagini 2D
							e stima delle loro posizioni 3D attraverso un modello di ML
							addestrato sul set di dati <a href="https://google.github.io/mediapipe/solutions/objectron.html" target="_blank">MediaPipe Objectron</a>. Le immagini 2D vengono
							prelevate dalle nuvole di punti (clouds points).
							<br>
							Per analizzare questo problema il presente elaborato progettuale consta di due parti:
							<ol>
								<li>Simulazione del robot Franka Emika Panda</li>
								<li>Detection della posizione degli oggetti</li>
							<ol>
							</p>
                        </div><!--//content-->
                    </div><!--//section-inner-->                 
                </section><!--//section-->
				
				<section class="about section">
                    <div class="section-inner">
                        <h2 class="heading">Motivazioni</h2>
                        <div class="content">						
							<p align = "justify">La detection di oggetti è un problema di computer vision ampiamente studiato 	
							ma la maggior parte della ricerca si è concentrata sulla previsione di oggetti 2D, 
							fornendo solo riquadri di delimitazione 2D.
							<br>
							Estendendo la previsione al 3D è possibile acquisire le dimensioni, la posizione e l'orientamento di un oggetto nel mondo,
							portando ad una varietà di applicazioni nella robotica, nei veicoli a guida autonoma, 
							nel recupero di immagini e nella realtà aumentata. <br> Un esempio è la simulazione
							del grasping utilizzando il robot Franka Emika Panda, questa può risultare molto utile in ambiti come l'industria 
							(es. robot collaborativi in grado di afferrare gli oggetti con la giusta posa nelle catene di montaggio).
							<br>
							La detection di oggetti 3D da immagini 2D è un problema impegnativo 
							a causa della mancanza di dati e della diversità di aspetti e forme degli oggetti all'interno di una categoria.
							</p>
							
							<iframe width = "640" height = "480" 
								src = "https://www.youtube.com/embed/iOrY1Uy6o04">
							</iframe>
							
                        </div><!--//content-->
                    </div><!--//section-inner-->                 
                </section><!--//section-->
			
                <section class="about section">
                    <div class="section-inner">
                        <h2 class="heading"><a id="goals"></a>Obiettivi</h2>
                        <div class="content">
							<ol>
								<li>Simulazione del robot Franka Emika Panda </li>
								<li>Cattura di immagini 2D da nuvole di punti </li>
								<li>Stima della posa attraverso <a href="https://google.github.io/mediapipe/solutions/objectron.html" target="_blank">MediaPipe Objectron</a></li>
								<li>Grasping attraverso il robot in Gazebo</li>
							</ol>                  
                        </div><!--//content-->
                    </div><!--//section-inner-->                 
                </section><!--//section-->
				
								
				<section class="about section">
                    <div class="section-inner">
                        <h2 class="heading"><a id="dataset"></a>Implementazione</h2>
                        <div class="content">
                            <p>Le tecnologie utilizzate sono state:</p>
							<ul>
							    <li>Linguaggi:</li>
								<ul>
									<li>C++</li>
									<li>Python 3.7</li>
								</ul>
								<li>Google Colab</li>
								<li><a href="https://google.github.io/mediapipe/solutions/objectron.html" target="_blank">MediaPipe Objectron</a></li>
								<li><a href="http://wiki.ros.org/noetic" target="_blank">ROS Noetic</a></li>
								<li><a href="http://opencv.org/" target="_blank">OpenCV</a></li>		
								<li><a href="https://gazebosim.org/home" target="_blank">Gazebo</a></li>
								<li><a href="http://docs.ros.org/en/kinetic/api/moveit_tutorials/html/doc/getting_started/getting_started.html" target="_blank">MoveIt Plugin</a></li>
								<li><a href="http://wiki.ros.org/rviz" target="_blank">RViz</a></li>
								<li><a href="https://www.blender.org/" target="_blank">Blender</a></li>
                            </ul>	                                                   
                        </div><!--//content-->
                    </div><!--//section-inner-->                 
                </section><!--//section-->
				
				
				<section class="about section">
                    <div class="section-inner">
                        <h2 class="heading"><a id="dataset"></a>Metodi</h2>
                        <div class="content" >
							<h3>Creazione ambiente di lavoro</h3>
							<p> Inizialmente è stato creato un ambiente di lavoro (catkin) in <a href="http://wiki.ros.org/noetic" target="_blank">ROS Noetic</a>
								<br>
								<code>
								(Nella cartella progetto)
								<br>
								source /opt/ros/noetic/setup.bash
								<br>
								mkdir -p catkin_ws/src
								<br>
								cd catkin_ws/src/
								<br>
								catkin_init_workspace
								<br>
								cd ..
								<br>
								catkin_make
								<br>
								source devel/setup.bash
								</code>
							</p>
							<h3>Installazione dipendenze</h3>
							<p align = "justify">	
								Successivamente sono state installate tutte le varie dipendenze e packages attraverso il tutorial fornito dal
								sito relativo al robot Franka Emika.
								<br>
								Oltre alla visualizzazione in <a href="https://gazebosim.org/home" target="_blank">Gazebo</a> ed in <a href="http://wiki.ros.org/rviz" target="_blank">RViz</a>, è stato installato il <a href="http://docs.ros.org/en/kinetic/api/moveit_tutorials/html/doc/getting_started/getting_started.html" target="_blank">Plugin MoveIt</a> per facilitare i movimenti.
								<br>
								All'estremità del braccio è stato posto un sensore Realsense R200 affinchè si possano effettuare delle acquisizioni di nuvole di punti degli oggetti in <a href="https://gazebosim.org/home" target="_blank">Gazebo</a>.
								<br>
								I link delle varie installazioni sono riportati nel settore relativo ai 
								<a href="#Riferimenti" >Riferimenti</a>.
							</p>
							<h3>Simulazione del robot</h3>
							<p align = "justify">
							    <code>
								roslaunch panda_moveit_config demo_gazebo.launch
								<br>
								</code>
								Avviando il file launch si avvia il simulatore <a href="https://gazebosim.org/home" target="_blank">Gazebo</a> nel cui mondo troveremo i modelli di Franka Emika Panda, tavolo, lastre di appoggio, tazza, illuminazione basata sul sole ed il sensore Realsense R200. <br>
								Il tavolo è stato creato tramite un oggetto box, mentre la tazza è stata creata utilizzando il software <a href="https://www.blender.org/" target="_blank">Blender</a>. <br>
								Inoltre verrà avviato il visualizzatore 3D <a href="http://wiki.ros.org/rviz" target="_blank">RViz</a> in cui è possibile visualizzare il robot e la nuvola di punti ricavata dal sensore Realsense R200.
								Da <a href="http://wiki.ros.org/rviz" target="_blank">RViz</a> è possibile abilitare il <a href="http://docs.ros.org/en/kinetic/api/moveit_tutorials/html/doc/getting_started/getting_started.html" target="_blank">Plugin MoveIt</a>, avviato nel file launch, con cui si effettueranno i vari spostamenti.
								<div>
								<img 
								src="image/gazebo.png"
								height ="400" width ="800">
								</div>
								<br>
								<div>
								<img 
								src="image/RViz.png"
								height ="400" width ="800">
								</div>
								<br>
								<div>
								<img 
								src="image/rosgraph_demo_gazebo.png"
								height ="200" width ="800">
								</div>
							</p>						
							<h3>Comandi del robot</h3>
							<p>
								<code>
								rosrun moveit_commander moveit_commander_cmdline.py
								<br>
								</code>
								<a href="http://docs.ros.org/en/kinetic/api/moveit_tutorials/html/doc/getting_started/getting_started.html" target="_blank">MoveIt Plugin</a> offre un pacchetto python che gestisce in maniera semplice
								la pianificazione del movimento, il calcolo dei percorsi cartesiani ed altre funzionalità. In particolare, è stata utilizzata l'interfaccia
								a riga di comando in cui sono state impartite le varie traiettorie che il robot effettua nell'esperimento.
								<br>
								<code>
								use panda_arm
								<br>
								go forward/left/up/down/right/backward -distance-
								<br>
								</code>
								<div>
								<img 
								src="image/cmdline_help.jpg"
								height ="400" width ="800">
								</div>
							</p>
							<h3>Detection clouds points</h3>
							<p>
								Usando i comandi sopra citati si pone il manipolatore al di sopra dell'oggetto interessato (tazza) in modo da essere inquadrato dal sensore.
								<br>
								<code>
								rosrun realsense_gazebo_plugin SavePointCloud
								<br>
								</code>
								Attraverso questo script si riesce a ricavare un'immagine RGB dalla nuvola di punti.
								<div>
								<img 
								src="image/Capture.jpg"
								height ="480" width ="640">
								</div>
								Ora è possibile determinare la posa della tazza presente in <a href="https://gazebosim.org/home" target="_blank">Gazebo</a> attraverso <a href="https://google.github.io/mediapipe/solutions/objectron.html" target="_blank">MediaPipe Objectron</a>.	
								<br>
								<code>
								python3 cupobjectrondetection.py
								</code>
								<div>
								<img 
								src="image/annotated_image.jpg"
								height ="480" width ="640">
								</div>
                                Questo script ci restituisce un'immagine con la posizione della tazza nello spazio attraverso boundary box 3D ed il
                                relativo orientamento degli assi.
								<br>
								<a href="#Approfondimento" >Approfondimento</a>
							</p>
							<h3>Grasping</h3>
							<p>
								Una volta determinata la possibilità di prese dell'oggetto il robot viene posizionato al di sopra della tazza e attraverso l'imposizione di comandi è in grado di  aprire le pinze.
								<br>
								<code>
								rostopic pub --once /franka_gripper/move/goal franka_gripper/MoveActionGoal "goal: { width: 0.08, speed: 0.1 }"
								</code>
								<br>
								Dopo aver aperto le pinze il robot inizia la discesa fino all'altezza della tazza, posizione dalla quale si effetua il grasping tramite la chiusura della pinza e l'applicazione della forza lavoro.
								<br>
								<code>
								rostopic pub --once /franka_gripper/grasp/goal franka_gripper/GraspActionGoal "goal: { width: 0.04, epsilon:{ inner: 0.02, outer: 0.02 }, speed: 0.05, force: 5.0}"
								</code>
								<br>
								Per verificare la tenuta della presa è possibile abilitare il 'contacts' in <a href="https://gazebosim.org/home" target="_blank">Gazebo</a> che evidenzia i punti in cui è applicata la forza.
								<br> 
								Per concludere la simulazione la tazza viene spostata sulla lastra blu, come è ripostato nei 
								<a href="#Risultati" >risultati</a>.
								<div>
								<img 
								src="image/Grasping.png"
								height ="500" width ="700">
								</div>							
							</p>	
                        </div><!--//content-->
                    </div><!--//section-inner-->                 
                </section><!--//section-->
				

				
				<section class="about section">
                    <div class="section-inner">
                        <h2 class="heading"><a id="codice"></a>Codice</h2>
                        <div class="content">

							<div class="item">
								<a href="https://github.com/ClaudioCazzetta/Pose-3D-with-Gazebo" target="_blank">Link repository GitHub</a>
                            </div>
							
                                                     
                        </div><!--//content-->
                    </div><!--//section-inner-->                 
                </section><!--//section-->
				
				
				<section class="about section">
                    <div class="section-inner">
						<a name = "Risultati qualitativi" > </a>
                        <h2 class="heading"><a id="training"></a>Risultati qualitativi</h2>
                        <div class="content">
                        	<p>
                            	Il seguente video mostra una presa della tezza dal robot Franka Emika Panda in simulazione su Gazebo.
                            </p>
							<br>

							<video width = "640" height = "480" controls>
								<source src = "image/trajectory.mp4" type = "video/mp4">
							</video>	
							<p>
							<br>
								Il video seguente mostra ciò che viene visualizzato dal sensore sovracitato in RGB.
							<video width = "640" height = "480" controls>
								<source src = "image/Rgb.MP4" type = "video/mp4">
							</video>	
							</p>
							<br>
							<p>
								Infine, viene mostrata in blender la nuvola di punti da diverse angolazioni.
							<video width = "640" height = "480" controls>
								<source src = "image/Blender Nuvola di punti.mp4" type = "video/mp4">
							</video>	
							</p>
							
                        </div><!--//content-->
                    </div><!--//section-inner-->                 
                </section><!--//section-->
                
                <section class="about section">
                    <div class="section-inner">
						<a name = "Risultati quantitativi" > </a>
                        <h2 class="heading"><a id="training"></a>Risultati quantitativi</h2>
                        <div class="content">
                        	<p>
                            	Sono stati effettuati diverse traiettorie con diverse posizioni dell'oggetto in cui sono stati verificati sia successi (come sopra mostrato) che insuccessi.
                            	Gli insuccessi riscontrati possono dipendere da diverse cause:
                            	<ol> 
                            		<li>Posizione dell'oggetto: in determinato posizioni la tazza non viene riconosciuta in maniera adeguata dal detector oppure perde stabilità nella simulazione;</li>
                            		<li>Forza della presa: in base alle caratteristiche fisiche assegnate all'obj va regolata l'intensità della forza delle pinze del manipolatore;</li>
                            		<li>Gradi di libertà dei giunti del manipolatore.</li>
                            	</ol>

                            </p>
							
                        </div><!--//content-->
                    </div><!--//section-inner-->                 
                </section><!--//section-->
                
				<section class="about section">
                    <div class="section-inner">
                        <h2 class="heading"> Approfondimento</h2>
						<a name = "Approfondimento" > </a>
						<div class="content">
							<p align = "justify">
							<a href="https://google.github.io/mediapipe/solutions/objectron.html" target="_blank">MediaPipe Objectron</a> rileva oggetti in immagini 2D e stima le loro pose attraverso un modello di ML addestrato 
							sul set di dati Objectron. Il set di dati Objectron è una raccolta di brevi videoclip incentrati sugli oggetti. 
							In ogni video la telecamera si muove attorno all'oggetto catturandolo da diverse angolazioni. 
							I dati contengono anche riquadri di delimitazione 3D annotati manualmente per ciascun oggetto che descrivono la posizione, l'orientamento e le dimensioni dello stesso. 
							Il set di dati sulla tazza  è costituito da 2204 videoclip integrati con 546k annotazioni.
							MediaPipe inoltre fornisce anche un'api da cui è stato ricavato lo script utilizzato.
							<br>
							Nel Colab creato c'è la possibilità di effettuare la detection
							sia su immagini statiche che su video. Inoltre è possibile settare il numero di oggetti da individuare. Come in ogni detection è possibile configurare
							il valore di confidenza (detection confidence) che permette di stabilire se il rilevamento è avvenuto correttamente o meno. Si può impostare anche il
							tracking confidence che stabilisce la robustezza della soluzione (valori più alti di tracking confidence forniscono una robustezza maggiore a discapito 
							della velocità di esecuzione).
							<div class="item">
								<a href="https://github.com/ClaudioCazzetta/Pose-3D-with-Gazebo/blob/main/cupobjectrondetection.py" target="_blank">Cup Detection</a>
                            </div>
							</p>
                                                     
						</div><!--//content-->
                    </div><!--//section-inner-->                 
                </section><!--//section-->
              
				
            </div><!--//primary-->
            
			<div class="secondary col-md-4 col-sm-12 col-xs-12">
                <aside class="info aside section">
                    <div class="section-inner">
                        <h2 class="heading">Autori</h2>
                        <div class="content">
                            <p>Claudio Cazzetta</p>
							<p>Rocchina Valanzano</p>
                        </div><!--//content-->
                    </div><!--//section-inner-->                 
                </aside><!--//aside-->                        
								
                             
                <aside class="blog aside section">
                    <div class="section-inner">
						<a name = "Riferimenti" > </a>
                        <h2 class="heading"> Riferimenti </h2>
						<div class="content">
							
							<div class="item">
								<a href="http://web.unibas.it/bloisi/corsi/visione-e-percezione.html"
								target="_blank">VeP pagina del corso</a>
							</div>
							<div class="item">
								<a href="http://web.unibas.it/bloisi/" target="_blank">Domenico Bloisi's home page</a>
							</div>
							<div class="item">
								<a href="https://frankaemika.github.io/docs/installation_linux.html" target="_blank">Franka_ROS e Libfranka</a>
							</div>

							<div class="item">
								<a href="http://opencv.org/" target="_blank">OpenCV</a>
                            </div>
							
							<div class="item">
								<a href="https://google.github.io/mediapipe/solutions/objectron.html" target="_blank">MediaPipe Objectron</a>
                            </div>
							
							<div class="item">
								<a href="http://docs.ros.org/en/kinetic/api/moveit_tutorials/html/doc/getting_started/getting_started.html" target="_blank">MoveIt Plugin</a>
							</div>
							
							<div class="item">
								<a href="https://github.com/SnehalD14/realsense_gazebo_plugin" target="_blank">RealSense_Gazebo</a>
							</div>
							
							<div class="item">
								<a href="https://www.blender.org/" target="_blank">Blender</a>
							</div>
							
							<div class="item">
								<a href="https://gazebosim.org/home" target="_blank">Gazebo</a>
							</div>
							
							<div class="item">
								<a href="http://wiki.ros.org/rviz" target="_blank">RViz</a>
							</div>
							
                        </div><!--//content-->
                    </div><!--//section-inner-->
                </aside><!--//section--> 


            </div><!--//secondary-->    
        </div><!--//row-->
    </div><!--//masonry-->
    
    <!-- ******FOOTER****** --> 
    <footer class="footer">
        <div class="container text-center">
                <small class="copyright">This template adapted from <a href="http://themes.3rdwavemedia.com/" target="_blank">3rd Wave Media</a></small>
        </div><!--//container-->
    </footer><!--//footer-->
 
 

 

</body></html>