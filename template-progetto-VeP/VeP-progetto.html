<!DOCTYPE html>
<!-- VeP http://web.unibas.it/bloisi/corsi/visione-e-percezione.html -->
<html lang="en"><!--<![endif]--><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Object 3D</title>
	<!-- Meta -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Descrizione del progetto">
    <meta name="author" content="Domenico Bloisi adapted a 3rd Wave Media template">    
    <link rel="shortcut icon" href="http://web.unibas.it/bloisi/tutorial/favicon.ico">  
    <link href="VeP-progetto_files/css.txt" rel="stylesheet" type="text/css">
    <link href="VeP-progetto_files/css1.txt" rel="stylesheet" type="text/css"> 
    <!-- Global CSS -->
    <link rel="stylesheet" href="VeP-progetto_files/bootstrap.css">   
    <!-- Plugins CSS -->
    <link rel="stylesheet" href="VeP-progetto_files/font-awesome.css">
        
    <!-- Theme CSS -->  
    <link id="theme-style" rel="stylesheet" href="VeP-progetto_files/styles.css">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->
    
</head> 

<body>
    <!-- ******HEADER****** --> 
    <header class="header">
        <div class="container">                       
            <img class="profile-image img-responsive pull-left" src="" alt="Inserire immagine qui" hegiht="100" width="100">
            <div class="profile-content pull-left">
                <h1 class="name">Object 3D</h1>
                <h2 class="desc">Tutorial in MediaPipe e grasping con Franka Emika</h2>
            </div>

            <div class="profile-content pull-right">
				<img class="profile-image img-responsive pull-left"
				    src="http://web.unibas.it/bloisi/assets/images/logo.png"
					alt="unibas logo"
					height=97 width=312/>
				
				<p>&nbsp;</p>
				<h3 class="desc">
				<a href="http://web.unibas.it/bloisi/corsi/visione-e-percezione.html"
				target="_blank">
				Corso di Visione e Percezione</a>
				</h3>
			</div>
			
        </div><!--//container-->
    </header><!--//header-->
    
    <div class="container sections-wrapper">
        <div class="row">
            <div class="primary col-md-8 col-sm-12 col-xs-12">
			
			    <section class="about section">
                    <div class="section-inner">
                        <h2 class="heading">Problema</h2>
                        <div class="content">
                            <p align = "justify">Rilevamento di oggetti in immagini 2D
							e stima delle loro posizioni 3D attraverso un modello di ML
							addestrato sul set di dati Objectron. Le immagini 2D vengono
							prelevate dalle nuvole di punti (clouds points).
							<br>
							Per analizzare questo problema il presente elaborato progettuale consta di due parti:
							<ol>
								<li>Simulazione del robot Franka Emika Panda</li>
								<li>Detection della posizione degli oggetti</li>
							<ol>
							</p>
                        </div><!--//content-->
                    </div><!--//section-inner-->                 
                </section><!--//section-->
				
				<section class="about section">
                    <div class="section-inner">
                        <h2 class="heading">Motivazioni</h2>
                        <div class="content">						
							<p align = "justify">La detection di oggetti è un problema di computer vision ampiamente studiato, 
							ma la maggior parte della ricerca si è concentrata sulla previsione di oggetti 2D, 
							fornendo solo riquadri di delimitazione 2D.
							<br>
							Estendendo la previsione al 3D,è possibile acquisire le dimensioni, la posizione e l'orientamento di un oggetto nel mondo,
							portando a una varietà di applicazioni nella robotica, nei veicoli a guida autonoma, 
							nel recupero di immagini e nella realtà aumentata. Un esempio è la simulazione
							del grasping utilizzando il robot Franka Emika Panda, questa può risultare molto utile in ambiti come l'industria 
							(ad esempio, robot collaborativi in grado di afferrare gli oggetti con la giusta pose nelle catene di montaggio).
							<br>
							La detection di oggetti 3D da immagini 2D è un problema impegnativo 
							a causa della mancanza di dati e della diversità di aspetti e forme degli oggetti all'interno di una categoria.
							</p>

							
                        </div><!--//content-->
                    </div><!--//section-inner-->                 
                </section><!--//section-->
			
                <section class="about section">
                    <div class="section-inner">
                        <h2 class="heading"><a id="goals"></a>Obiettivi</h2>
                        <div class="content">
							<ol>
								<li>Simulazione del robot Franka Emika Panda </li>
								<li>Cattura di immagini 2D da clouds points </li>
								<li>Stima della posa attraverso Objectron di MediaPipe</li>
								<li>Grasping attraverso il robot in Gazebo</li>
							</ol>                  
                        </div><!--//content-->
                    </div><!--//section-inner-->                 
                </section><!--//section-->
				
								
				<section class="about section">
                    <div class="section-inner">
                        <h2 class="heading"><a id="dataset"></a>Implementazione</h2>
                        <div class="content">
                            <p>Le tecnologie utilizzate sono state:</p>
							<ul>
							    <li>Linguaggi:</li>
								<ul>
									<li>C++</li>
									<li>Python 3.7</li>
								</ul>
								<li>Google Colab</li>
								<li>MediaPipe</li>
								<li>ROS Noetic</li>
								<li>OpenCV</li>		
								<li>Gazebo</li>
								<li>MoveIt</li>
								<li>RViz</li>
								<li>Blender</li>
                            </ul>	                                                   
                        </div><!--//content-->
                    </div><!--//section-inner-->                 
                </section><!--//section-->
				
				
				<section class="about section">
                    <div class="section-inner">
                        <h2 class="heading"><a id="dataset"></a>Metodi</h2>
                        <div class="content">
							<h3>Creazione ambiente di lavoro</h3>
							<p> Inizialmente è stato creato un ambiente di lavoro (catkin) in ROS Noetic 
								<code>
								<br>
								(Nella cartella progetto)
								<br>
								source /opt/ros/noetic/setup.bash
								<br>
								mkdir -p catkin_ws/src
								<br>
								cd catkin_ws/src/
								<br>
								catkin_init_workspace
								<br>
								cd ..
								<br>
								catkin_make
								<br>
								source devel/setup.bash
								</code>
							</p>
							<h3>Installazione dipendenze</h3>
							<p>	
								Successivamente sono state installate tutte le varie dipendenze e packages attraverso il tutorial fornito dal
								sito relativo al robot Franka Emika.
								<br>
								Oltre alla visualizzazione in Gazebo ed in RViz, è stato installato il plugin MoveIt per facilitare i movimenti
								<br>
								All'estremita' del braccio è stato posto un sensore Realsense R200 affinchè si possano effettuare delle acquisizioni di nuvole di punti degli oggetti in Gazebo
								<br>
								I link delle varie installazioni sono riportati nel settore relativo ai 
								<a href="#Riferimenti" >Riferimenti</a>
							</p>
							<h3>Simulazione del robot</h3>
							<p>
							    <code>
								roslaunch panda_moveit_config demo_gazebo.launch
								<br>
								</code>
								Avviando il file launch si avvia il simulatore Gazebo nel cui mondo troveremo i modelli di Franka Emika Panda,tavolo,lastre,tazza,illuminazione basata sul sole
								ed il sensore Realsense R200.
								Il tavolo è stato creato tramite un oggetto box, mentre la tazza è stata creata utilizzando il software Blender.
								Inoltre verrà avviato il visualizzatore 3D RViz in cui è possibile visualizzare il robot e la nuvola di punti ricavata dal sensore Realsense R200.
								Da RViz è possibile abilitare il plugin MoveIt(avviato nel file launch) con cui si effettueranno i vari spostamenti.
								<div>
								<img 
								src="image/gazebo.png"
								height ="400" width ="800">
								</div>
								<div>
								<img 
								src="image/RViz.png"
								height ="400" width ="800">
								</div>
								<div>
								<img 
								src="image/rosgraph_demo_gazebo.png"
								height ="400" width ="800">
								</div>
							</p>						
							<h3>Comandi del robot</h3>
							<p>
								<code>
								rosrun moveit_commander moveit_commander_cmdline.py
								<br>
								</code>
								MoveIt offre un pacchetto python che gestisce in maniera semplice
								la pianificazione del movimento, il calcolo dei percorsi cartesiani ed altre funzionalità.In particolare,è stata utilizzata l'interfaccia
								a riga di comando in cui sono state impartite le varie traiettorie che il robot effettua nell'esperimento.
								<code>
								<br>
								use panda_arm
								<br>
								go forward/left/up/down/right/backward -distance-
								<br>
								</code>
								<div>
								<img 
								src="image/cmdline_help.jpg"
								height ="400" width ="800">
								</div>
							</p>
							<h3>Detection clouds points</h3>
							<p>
								Usando i comandi sopra citati, si pone il manipolatore al di sopra dell'oggetto interessato(tazza) in modo da essere inquadrato dal sensore.
								<code>
								<br>
								rosrun realsense_gazebo_plugin SaveCloudPoint
								<br>
								</code>
								Attraverso questo script si riesce a ricavare un'immagine RGB dalla nuvola di punti.
								<div>
								<img 
								src=""
								height ="400" width ="800">
								</div>
								Ora è possibile determinare la posa della tazza presente in Gazebo attraverso Objectron di MediaPipe
								<code>
								<br>
								python3 cupobjectrondetection.py
								</code>
								<div>
								<img 
								src=""
								height ="400" width ="800">
								</div>
                                Questo script ci restituisce un'immagine con la posizione della tazza nello spazio attraverso boundary box 3D,
								posizione e orientamento degli assi.
								<br>
								<a href="#Approfondimento" >Approfondimento</a>
							</p>
							<h3>Grasping</h3>
							<p>
							Mancano immagini nuvola di punti + immagine annotata. Da fare tutto il grasping e come immagini la presa. Da checkare i punti di contatto
							Da ricordarsi di inserire comandi apri pinza + comandi chiudi pinza .
							
							</p>	
                        </div><!--//content-->
                    </div><!--//section-inner-->                 
                </section><!--//section-->
				

				
				<section class="about section">
                    <div class="section-inner">
                        <h2 class="heading"><a id="codice"></a>Codice</h2>
                        <div class="content">

							<div class="item">
								<a href="https://github.com/ClaudioCazzetta/Pose-3D-with-Gazebo" target="_blank">Link repository GitHub</a>
                            </div>
							
                                                     
                        </div><!--//content-->
                    </div><!--//section-inner-->                 
                </section><!--//section-->
				
				
				<section class="about section">
                    <div class="section-inner">
                        <h2 class="heading"><a id="dataset"></a>Dataset</h2>
                        <div class="content">
                            <p>Descrivere i dati utilizzati fornendo i link per il download</p>
                            							
                                                     
                        </div><!--//content-->
                    </div><!--//section-inner-->                 
                </section><!--//section-->
				
				<section class="about section">
                    <div class="section-inner">
                        <h2 class="heading"><a id="training"></a>Risultati</h2>
                        <div class="content">
                            							
								
							<h3>Risultati qualitativi</h3>
							<p>Fornire esempi di applicazione del metodo con immagini e video</p>
							
													
							<h3>Risultati quantitativi</h3>
							<p>Fornire una descrizione delle performance del metodo usando
							delle opportune metriche</p>
							
							<p>Per esempio:<br>
							Fornire su un numero di immagini non inferiore a 30 i dati relativi
							a FP, FN, TP.							
							</p>
							
							
							
                        </div><!--//content-->
                    </div><!--//section-inner-->                 
                </section><!--//section-->
                
				<section class="about section">
                    <div class="section-inner">
                        <h2 class="heading"> Approfondimento</h2>
						<a name = "Approfondimento" > </a>
						<div class="content">
							<p align = "justify">
							MediaPipe Objectron rileva oggetti in immagini 2D e stima le loro pose attraverso un modello di ML addestrato 
							sul set di dati Objectron. Il set di dati Objectron è una raccolta di brevi video clip incentrati sugli oggetti. 
							In ogni video, la telecamera si muove attorno all'oggetto, catturandolo da diverse angolazioni. 
							I dati contengono anche riquadri di delimitazione 3D annotati manualmente per ciascun oggetto, che descrivono la posizione, l'orientamento e le dimensioni dell'oggetto. 
							Il set di dati sulla tazza  è costituito da 2204 video clip annotati integrati con 546k annotazioni.
							MediaPipe,inoltre, fornisce anche un api da cui è stato ricavato il Colab utilizzato.
							<br>
							Nel Colab creato c'è la possibilità di effettuare la detection
							sia su immagini statiche e sia su video.Inoltre è possibile settare il numero di oggetti da individuare. Come in ogni detection è possibile configurare
							il valore di confidenza (detection confidence) che permette di stabilire se il rilevamento è avvenuto correttamente o meno. Si può impostare anche il
							tracking confidence che stabilisce la robustezza della soluzione (valori più alti di tracking confidence forniscono una robustezza maggiore a discapito 
							della velocità di esecuzione).
							<div class="item">
								<a href="" target="_blank">Cup Detection Colab</a>
                            </div>
							</p>
                                                     
						</div><!--//content-->
                    </div><!--//section-inner-->                 
                </section><!--//section-->
              
				
            </div><!--//primary-->
            
			<div class="secondary col-md-4 col-sm-12 col-xs-12">
                <aside class="info aside section">
                    <div class="section-inner">
                        <h2 class="heading">Autori</h2>
                        <div class="content">
                            <p>Claudio Cazzetta</p>
							<p>Rocchina Valanzano</p>
                        </div><!--//content-->
                    </div><!--//section-inner-->                 
                </aside><!--//aside-->                        
								
                             
                <aside class="blog aside section">
                    <div class="section-inner">
                        <h2 class="heading"> Riferimenti </h2>
						<a name = "Riferimenti" > </a>
						<div class="content">
							
							<div class="item">
								<a href="http://web.unibas.it/bloisi/corsi/visione-e-percezione.html"
								target="_blank">VeP pagina del corso</a>
							</div>
							<div class="item">
								<a href="http://web.unibas.it/bloisi/" target="_blank">Domenico Bloisi's home page</a>
							</div>
							<div class="item">
								<a href="https://frankaemika.github.io/docs/installation_linux.html" target="_blank">Franka_ROS e Libfranka</a>
							</div>

							<div class="item">
								<a href="http://opencv.org/" target="_blank">OpenCV</a>
                            </div>
							
							<div class="item">
								<a href="https://google.github.io/mediapipe/solutions/objectron.html" target="_blank">MediaPipe Objectron</a>
                            </div>
							
							<div class="item">
								<a href="http://docs.ros.org/en/kinetic/api/moveit_tutorials/html/doc/getting_started/getting_started.html" target="_blank">MoveIt Plugin</a>
							</div>
							
							<div class="item">
								<a href="https://github.com/SnehalD14/realsense_gazebo_plugin" target="_blank">RealSense_Gazebo</a>
							</div>
							
							<div class="item">
								<a href="https://www.blender.org/" target="_blank">Blender</a>
							</div>
							
                        </div><!--//content-->
                    </div><!--//section-inner-->
                </aside><!--//section--> 


            </div><!--//secondary-->    
        </div><!--//row-->
    </div><!--//masonry-->
    
    <!-- ******FOOTER****** --> 
    <footer class="footer">
        <div class="container text-center">
                <small class="copyright">This template adapted from <a href="http://themes.3rdwavemedia.com/" target="_blank">3rd Wave Media</a></small>
        </div><!--//container-->
    </footer><!--//footer-->
 
 

 

</body></html>